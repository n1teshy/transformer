{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/n1teshy/transformer > /dev/null\n",
    "!mv transformer/* . && rmdir transformer\n",
    "!mkdir -p drive/MyDrive/checkpoints/poet2\n",
    "!ls drive/MyDrive/checkpoints/poet2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from core.data.generator import GeneratorDataset\n",
    "from core.models import Generator\n",
    "from core.utils.bpe import Tokenizer\n",
    "from core.utils.configs import DecoderConfig, GeneratorDataConfig\n",
    "from core.utils.loss import LossMonitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data conf\n",
    "batch_size = 8\n",
    "context = 512\n",
    "train_cache = \"drive/MyDrive/datasets/poems_cache_34k/train\"\n",
    "val_cache = \"drive/MyDrive/datasets/poems_cache_34k/val\"\n",
    "sample_delimiter = (\"# \" * 39) + \"#\"\n",
    "\n",
    "# model conf\n",
    "no_blocks = 1\n",
    "no_heads = 1\n",
    "model_dim = 512\n",
    "model_context = 512\n",
    "\n",
    "# training conf\n",
    "epochs = 10\n",
    "learning_rate = 0.001\n",
    "min_loss_improv = 1\n",
    "loss_window = 128\n",
    "checkpoints_dir = \"drive/MyDrive/checkpoints/poet2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.load(\"tokenizer/poet2_tokenizer.model\")\n",
    "\n",
    "train_data_conf = GeneratorDataConfig(\n",
    "    batch_size=batch_size, pad_id=tokenizer.pad_id, cache_dir=train_cache\n",
    ")\n",
    "val_data_conf = GeneratorDataConfig(\n",
    "    batch_size=batch_size, pad_id=tokenizer.pad_id, cache_dir=val_cache\n",
    ")\n",
    "model_conf = DecoderConfig(\n",
    "    no_blocks=no_blocks,\n",
    "    no_heads=no_heads,\n",
    "    model_dim=model_dim,\n",
    "    vocab_size=tokenizer.size,\n",
    "    pad_id=tokenizer.pad_id,\n",
    "    context=model_context,\n",
    "    dropout=0.2,\n",
    "    train_mode=True,\n",
    "    sos_id=tokenizer.sos_id,\n",
    "    eos_id=tokenizer.eos_id,\n",
    ")\n",
    "\n",
    "train_dataset = GeneratorDataset(train_data_conf)\n",
    "val_dataset = GeneratorDataset(val_data_conf)\n",
    "model = Generator(model_conf)\n",
    "# model.load_state_dict(torch.load())\n",
    "model = model.to(\"cuda\")\n",
    "print(\n",
    "    \"model has %.2fmn parameters\"\n",
    "    % (sum(p.numel() for p in model.parameters()) / 1e6,)\n",
    ")\n",
    "loss_monitor = LossMonitor(\"train\", \"val\", window=loss_window)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_val_loss() -> float:\n",
    "    model.eval()\n",
    "    batch = val_dataset.next_batch()\n",
    "    if batch is None:\n",
    "        val_dataset.reset()\n",
    "        batch = val_dataset.next_batch()\n",
    "    x, y = batch\n",
    "    _, loss = model(x, y)\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def save_model(t_loss: float, v_loss: float):\n",
    "    name = \"%.2f-%.2f-%.2f-%d-%d-%d-%d.pth\" % (\n",
    "        t_loss,\n",
    "        v_loss,\n",
    "        learning_rate,\n",
    "        no_blocks,\n",
    "        no_heads,\n",
    "        model_dim,\n",
    "        model_context,\n",
    "    )\n",
    "    torch.save(model.state_dict(), os.path.join(checkpoints_dir, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grad_accum_steps, best_t_loss, best_v_loss = 1, None, None\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    batches_processed = 0\n",
    "    accum_t_loss = 0\n",
    "    while batch := train_dataset.next_batch():\n",
    "        model.train()\n",
    "        x, y = batch\n",
    "        _, loss = model(x, y)\n",
    "        batches_processed += 1\n",
    "        accum_t_loss += loss.item()\n",
    "        loss = loss / grad_accum_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        if batches_processed % grad_accum_steps == 0:\n",
    "            t_loss, v_loss = accum_t_loss / grad_accum_steps, get_val_loss()\n",
    "            losses = loss_monitor.update(train=t_loss, val=v_loss)\n",
    "            mt_loss, mv_loss = losses[\"train\"], losses[\"val\"]\n",
    "            print(\n",
    "                \"%d-%d. t-loss: %.2f -> %.2f, v-loss: %.2f -> %.2f\"\n",
    "                % (\n",
    "                    epoch,\n",
    "                    batches_processed / grad_accum_steps,\n",
    "                    t_loss,\n",
    "                    mt_loss,\n",
    "                    v_loss,\n",
    "                    mv_loss\n",
    "                )\n",
    "            )\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            if (\n",
    "                batches_processed >= loss_window\n",
    "                and best_t_loss - mt_loss >= min_loss_improv\n",
    "                and mv_loss - mt_loss < min_loss_improv\n",
    "            ):\n",
    "                save_model(mt_loss, mv_loss)\n",
    "                best_t_loss, best_v_loss = mt_loss, mv_loss\n",
    "                print(\"saved with losses: %.2f, %.2f\" % (mt_loss, mv_loss))\n",
    "            accum_t_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in model.generate():\n",
    "  print(tokenizer.decode([token]), end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
