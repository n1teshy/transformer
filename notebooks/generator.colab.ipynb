{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/n1teshy/transformer > /dev/null\n",
    "!mv transformer/* . && rmdir transformer\n",
    "!mkdir -p drive/MyDrive/checkpoints/poet2\n",
    "!ls drive/MyDrive/checkpoints/poet2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "\n",
    "import torch\n",
    "from torch.optim import AdamW\n",
    "\n",
    "from core.data.generator import GeneratorDataset\n",
    "from core.models import Generator\n",
    "from core.utils.bpe import Tokenizer\n",
    "from core.utils.configs import DecoderConfig, GeneratorDataConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data conf\n",
    "batch_size = 32\n",
    "context = 512\n",
    "train_cache = \"drive/MyDrive/datasets/poems_cache_34k/train\"\n",
    "val_cache = \"drive/MyDrive/datasets/poems_cache_34k/val\"\n",
    "sample_delimiter = (\"# \" * 39) + \"#\"\n",
    "\n",
    "# model conf\n",
    "no_blocks = 5\n",
    "no_heads = 16\n",
    "model_dim = 768\n",
    "model_context = 512\n",
    "\n",
    "# training conf\n",
    "epochs = 10\n",
    "checkpoints_dir = \"drive/MyDrive/checkpoints/poet2\"\n",
    "\n",
    "learning_rate = 3e-3\n",
    "min_lr = learning_rate / 10\n",
    "total_samples = 54685\n",
    "grad_accum_iters = 1\n",
    "warmup_iters = int(total_samples / batch_size / grad_accum_iters * 0.2)\n",
    "lr_decay_iters = int(total_samples / batch_size / grad_accum_iters * 0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()\n",
    "tokenizer.load(\"tokenizer/poet2_tokenizer.model\")\n",
    "\n",
    "train_data_conf = GeneratorDataConfig(\n",
    "    batch_size=batch_size,\n",
    "    pad_id=tokenizer.pad_id,\n",
    "    cache_dir=train_cache,\n",
    "    shuffle_shards=True,\n",
    "    shuffle_samples=True\n",
    ")\n",
    "val_data_conf = GeneratorDataConfig(\n",
    "    batch_size=batch_size,\n",
    "    pad_id=tokenizer.pad_id,\n",
    "    cache_dir=val_cache,\n",
    "    shuffle_shards=True,\n",
    "    shuffle_samples=True\n",
    ")\n",
    "\n",
    "train_dataset = GeneratorDataset(train_data_conf)\n",
    "val_dataset = GeneratorDataset(val_data_conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conf = DecoderConfig(\n",
    "    no_blocks=no_blocks,\n",
    "    no_heads=no_heads,\n",
    "    model_dim=model_dim,\n",
    "    vocab_size=tokenizer.size,\n",
    "    pad_id=tokenizer.pad_id,\n",
    "    context=model_context,\n",
    "    dropout=0.2,\n",
    "    train_mode=True,\n",
    "    sos_id=tokenizer.sos_id,\n",
    "    eos_id=tokenizer.eos_id,\n",
    ")\n",
    "model = Generator(model_conf)\n",
    "# model.load_state_dict(torch.load(os.path.join(checkpoints_dir, )))\n",
    "model = model.to(\"cuda\")\n",
    "print(\n",
    "    \"model has %.2fmn parameters\"\n",
    "    % (sum(p.numel() for p in model.parameters()) / 1e6,)\n",
    ")\n",
    "\n",
    "\n",
    "def get_lr(it: int):\n",
    "    if it < warmup_iters:\n",
    "        return learning_rate * (it + 1) / (warmup_iters + 1)\n",
    "    if it > lr_decay_iters:\n",
    "        return min_lr\n",
    "    decay_ratio = (it - warmup_iters) / (lr_decay_iters - warmup_iters)\n",
    "    assert 0 <= decay_ratio <= 1\n",
    "    coeff = 0.5 * (1.0 + math.cos(math.pi * decay_ratio))\n",
    "    return min_lr + coeff * (learning_rate - min_lr)\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def get_val_loss() -> float:\n",
    "    model.eval()\n",
    "    batch = val_dataset.next_batch()\n",
    "    if batch is None:\n",
    "        val_dataset.reset()\n",
    "        batch = val_dataset.next_batch()\n",
    "    x, y = batch\n",
    "    _, loss = model(x, y)\n",
    "    return loss.item()\n",
    "\n",
    "\n",
    "def save_model(t_loss: float, v_loss: float):\n",
    "    name = \"%.2f-%.2f-%.2f-%d-%d-%d-%d.pth\" % (\n",
    "        t_loss,\n",
    "        v_loss,\n",
    "        learning_rate,\n",
    "        no_blocks,\n",
    "        no_heads,\n",
    "        model_dim,\n",
    "        model_context,\n",
    "    )\n",
    "    torch.save(model.state_dict(), os.path.join(checkpoints_dir, name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_window, batches_trained = 128 / grad_accum_iters, 0\n",
    "mt_loss, mv_loss = None, None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_t_loss, best_v_loss = 4, 4\n",
    "min_loss_improv = 0.2\n",
    "\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    while batch := train_dataset.next_batch():\n",
    "        model.train()\n",
    "        x, y = batch\n",
    "        _, loss = model(x, y)\n",
    "        t_loss = loss.item()\n",
    "        loss /= grad_accum_iters\n",
    "        loss.backward()\n",
    "        batches_trained += 1\n",
    "        if batches_trained % grad_accum_iters == 0:\n",
    "            optimizer.zero_grad()\n",
    "            lr = get_lr(batches_trained / grad_accum_iters)\n",
    "            for group in optimizer.param_groups:\n",
    "                group[\"lr\"] = lr\n",
    "            optimizer.step()\n",
    "            v_loss = get_val_loss()\n",
    "            mt_loss = t_loss * (1/loss_window) + (mt_loss or t_loss) * (1 - 1/loss_window)\n",
    "            mv_loss = v_loss * (1/loss_window) + (mv_loss or v_loss) * (1 - 1/loss_window)\n",
    "            print(\n",
    "                \"%d-%d: %.2f -> %.2f, %.2f -> %.2f, lr: %.5f\"\n",
    "                % (epoch, batches_trained // grad_accum_iters, t_loss, mt_loss, v_loss, mv_loss, lr)\n",
    "            )\n",
    "            if (\n",
    "                (batches_trained // grad_accum_iters) >= loss_window\n",
    "                and best_t_loss - mt_loss >= min_loss_improv\n",
    "                and mv_loss - mt_loss < min_loss_improv\n",
    "            ):\n",
    "                save_model(mt_loss, mv_loss)\n",
    "                best_t_loss, best_v_loss = mt_loss, mv_loss\n",
    "                print(\"saved with losses: %.2f, %.2f\" % (mt_loss, mv_loss))\n",
    "        else:\n",
    "            print(\"# \" * (batches_trained % grad_accum_iters), end=\"\\r\")\n",
    "    train_dataset.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in model.generate():\n",
    "  print(tokenizer.decode([token]), end=\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cuda-torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
