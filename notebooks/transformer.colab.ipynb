{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/n1teshy/transformer > /dev/null\n",
    "!mv transformer/* . && rmdir transformer > /dev/null\n",
    "!ls drive/MyDrive/checkpoints/en-hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp drive/MyDrive/checkpoints/en-hi/ params.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.optim import AdamW\n",
    "from core.utils.bpe import Tokenizer\n",
    "from core.data.seq_to_seq import SeqToSeqDataset\n",
    "from core.utils.configs import SeqToSeqDataConfig, EncoderConfig, DecoderConfig\n",
    "from core.models import Transformer\n",
    "from core.utils.loss import LossMonitor\n",
    "from core.globals import DEVICE\n",
    "from core.constants import TOKEN_PAD, TOKEN_SOS, TOKEN_EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_SOURCE_FILE = Path(\"./drive/MyDrive/datasets/en-hi/train/\")\n",
    "# TRAIN_TARGET_FILE = Path(\"./drive/MyDrive/datasets/en-hi/train/\")\n",
    "# VAL_SOURCE_FILE = Path(\"./drive/MyDrive/datasets/en-hi/val/\")\n",
    "# VAL_TARGET_FILE = Path(\"./drive/MyDrive/datasets/en-hi/val/\")\n",
    "TRAIN_SOURCE = None\n",
    "TRAIN_TARGET = None\n",
    "VAL_SOURCE = None\n",
    "VAL_TARGET = None\n",
    "TRAIN_CACHE = Path(\"drive/MyDrive/datasets/en-hi/cached-train-6.9\")\n",
    "VAL_CACHE = Path(\"drive/MyDrive/datasets/en-hi/cached-val-0.9\")\n",
    "ENCODER_CONTEXT = 1024\n",
    "DECODER_CONTEXT = 512\n",
    "BATCH_SIZE = 64\n",
    "ENCODER_BLOCKS = 2\n",
    "ENCODER_HEADS = 4\n",
    "DECODER_BLOCKS = 2\n",
    "DECODER_HEADS = 4\n",
    "MODEL_DIM = 512\n",
    "\n",
    "assert MODEL_DIM % ENCODER_HEADS == MODEL_DIM % DECODER_HEADS == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer = Tokenizer()\n",
    "en_tokenizer.load(\"tokenizers/en.model\")\n",
    "hi_tokenizer = Tokenizer()\n",
    "hi_tokenizer.load(\"tokenizers/hi.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.data.circular import CircularBatchGenerator\n",
    "\n",
    "base_data_config = dict(\n",
    "    source=None,\n",
    "    target=None,\n",
    "    encoder_context=ENCODER_CONTEXT,\n",
    "    decoder_context=DECODER_CONTEXT,\n",
    "    encode_source=en_tokenizer.encode,\n",
    "    encode_target=hi_tokenizer.encode,\n",
    "    source_pad_id=en_tokenizer.specials[TOKEN_PAD],\n",
    "    target_pad_id=hi_tokenizer.specials[TOKEN_PAD],\n",
    "    sos_id=hi_tokenizer.specials[TOKEN_SOS],\n",
    "    eos_id=hi_tokenizer.specials[TOKEN_EOS],\n",
    "    batch_size=BATCH_SIZE\n",
    ")\n",
    "\n",
    "train_dataset = SeqToSeqDataset(\n",
    "    SeqToSeqDataConfig(\n",
    "        **dict(\n",
    "            base_data_config,\n",
    "            source=TRAIN_SOURCE,\n",
    "            target=TRAIN_TARGET,\n",
    "            cache_dir=TRAIN_CACHE\n",
    "        )\n",
    "    )\n",
    ")\n",
    "train_loader = train_dataset.batch_generator()\n",
    "\n",
    "val_dataset = SeqToSeqDataset(\n",
    "    SeqToSeqDataConfig(\n",
    "        **dict(\n",
    "            base_data_config,\n",
    "            source=VAL_SOURCE,\n",
    "            target=VAL_TARGET,\n",
    "            cache_dir=VAL_CACHE\n",
    "        )\n",
    "    )\n",
    ")\n",
    "val_loader = iter(CircularBatchGenerator(val_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_config = EncoderConfig(\n",
    "    no_blocks=ENCODER_BLOCKS,\n",
    "    no_heads=ENCODER_HEADS,\n",
    "    model_dim=MODEL_DIM,\n",
    "    vocab_size=en_tokenizer.size,\n",
    "    pad_id=en_tokenizer.specials[TOKEN_PAD],\n",
    "    context=ENCODER_CONTEXT\n",
    ")\n",
    "decoder_config = DecoderConfig(\n",
    "    no_blocks=DECODER_BLOCKS,\n",
    "    no_heads=DECODER_HEADS,\n",
    "    model_dim=MODEL_DIM,\n",
    "    vocab_size=hi_tokenizer.size,\n",
    "    pad_id=hi_tokenizer.specials[TOKEN_PAD],\n",
    "    context=DECODER_CONTEXT,\n",
    "    sos_id=hi_tokenizer.specials[TOKEN_SOS],\n",
    "    eos_id=hi_tokenizer.specials[TOKEN_EOS]\n",
    ")\n",
    "model = Transformer(encoder_config, decoder_config).to(DEVICE)\n",
    "# model.load_state_dict(torch.load(\"params.pth\", map_location=DEVICE))\n",
    "no_params = sum(p.nelement() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"model has {no_params / 1000 ** 2:.4f} million trainable parameters\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def calc_val_loss():\n",
    "    model.eval()\n",
    "    x, y = next(val_loader)\n",
    "    logits, loss = model(x, y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_monitor = LossMonitor(\"train\", \"val\", window=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_trained = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inp, tgt in train_loader:\n",
    "    model.train()\n",
    "    logits, t_loss = model(inp, tgt)\n",
    "    optimizer.zero_grad()\n",
    "    t_loss.backward()\n",
    "    optimizer.step()\n",
    "    t_loss, v_loss = t_loss.item(), calc_val_loss().item()\n",
    "    losses = loss_monitor.update(train=t_loss, val=v_loss)\n",
    "    batches_trained += 1\n",
    "    print(f\"{batches_trained} -> {losses['train']:.4f}, {losses['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient accumulation\n",
    "acc_t_loss = 0\n",
    "accumulation_steps = 3\n",
    "for inp, tgt in train_loader:\n",
    "    model.train()\n",
    "    logits, t_loss = model(inp, tgt)\n",
    "    acc_t_loss += t_loss.item()\n",
    "    t_loss = t_loss / accumulation_steps\n",
    "    t_loss.backward()\n",
    "    batches_trained += 1\n",
    "    if batches_trained % accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        t_loss, v_loss = acc_t_loss / accumulation_steps, calc_val_loss().item()\n",
    "        losses = loss_monitor.update(train=t_loss, val=v_loss)\n",
    "        print(f\"{batches_trained // accumulation_steps} -> {losses['train']:.4f}, {losses['val']:.4f}\")\n",
    "        acc_t_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "print(hi_tokenizer.decode(list(model.generate(torch.tensor([en_tokenizer.encode(\"\"\"Hello, how are you?\"\"\")])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"drive/MyDrive/checkpoints/en-hi/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
