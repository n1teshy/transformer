{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/n1teshy/transformer > /dev/null\n",
    "!mv transformer/* . && rmdir transformer > /dev/null\n",
    "!ls drive/MyDrive/checkpoints/en-hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp drive/MyDrive/checkpoints/en-hi/ params.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from torch.optim import AdamW\n",
    "from core.utils.bpe import Tokenizer\n",
    "from core.data.seq_to_seq import SeqToSeqDataset\n",
    "from core.utils.configs import SeqToSeqDataConfig, EncoderConfig, DecoderConfig\n",
    "from core.models import Transformer\n",
    "from core.utils.loss import LossMonitor\n",
    "from core.globals import DEVICE\n",
    "from core.constants import TOKEN_PAD, TOKEN_SOS, TOKEN_EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_SOURCE = Path(\"./drive/MyDrive/datasets/en-hi/train/\")\n",
    "TRAIN_TARGET = Path(\"./drive/MyDrive/datasets/en-hi/train/\")\n",
    "VAL_SOURCE = Path(\"./drive/MyDrive/datasets/en-hi/val/\")\n",
    "VAL_TARGET = Path(\"./drive/MyDrive/datasets/en-hi/val/\")\n",
    "TRAIN_CACHE = Path(\"drive/MyDrive/datasets/en-hi/cached-train-6.9\")\n",
    "VAL_CACHE = Path(\"drive/MyDrive/datasets/en-hi/cached-val-0.9\")\n",
    "ENCODER_CONTEXT = 1024\n",
    "DECODER_CONTEXT = 512\n",
    "BATCH_SIZE = 64\n",
    "ENCODER_BLOCKS = 2\n",
    "ENCODER_HEADS = 4\n",
    "DECODER_BLOCKS = 2\n",
    "DECODER_HEADS = 4\n",
    "MODEL_DIM = 512\n",
    "\n",
    "assert MODEL_DIM % ENCODER_HEADS == MODEL_DIM % DECODER_HEADS == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer = Tokenizer()\n",
    "en_tokenizer.load(\"tokenizers/en.model\")\n",
    "hi_tokenizer = Tokenizer()\n",
    "hi_tokenizer.load(\"tokenizers/hi.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_data_config = dict(\n",
    "    source=None,\n",
    "    target=None,\n",
    "    encoder_context=ENCODER_CONTEXT,\n",
    "    decoder_context=DECODER_CONTEXT,\n",
    "    encode_source=en_tokenizer.encode,\n",
    "    encode_target=hi_tokenizer.encode,\n",
    "    source_pad_id=en_tokenizer.specials[TOKEN_PAD],\n",
    "    target_pad_id=hi_tokenizer.specials[TOKEN_PAD],\n",
    "    sos_id=hi_tokenizer.specials[TOKEN_SOS],\n",
    "    eos_id=hi_tokenizer.specials[TOKEN_EOS],\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle_shards=True,\n",
    "    shuffle_samples=True\n",
    ")\n",
    "\n",
    "train_dataset = SeqToSeqDataset(\n",
    "    SeqToSeqDataConfig(\n",
    "        **dict(\n",
    "            base_data_config,\n",
    "            source=TRAIN_SOURCE,\n",
    "            target=TRAIN_TARGET,\n",
    "            cache_dir=TRAIN_CACHE\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "val_dataset = SeqToSeqDataset(\n",
    "    SeqToSeqDataConfig(\n",
    "        **dict(\n",
    "            base_data_config,\n",
    "            source=VAL_SOURCE,\n",
    "            target=VAL_TARGET,\n",
    "            cache_dir=VAL_CACHE\n",
    "        )\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_config = EncoderConfig(\n",
    "    no_blocks=ENCODER_BLOCKS,\n",
    "    no_heads=ENCODER_HEADS,\n",
    "    model_dim=MODEL_DIM,\n",
    "    vocab_size=en_tokenizer.size,\n",
    "    pad_id=en_tokenizer.specials[TOKEN_PAD],\n",
    "    context=ENCODER_CONTEXT\n",
    ")\n",
    "decoder_config = DecoderConfig(\n",
    "    no_blocks=DECODER_BLOCKS,\n",
    "    no_heads=DECODER_HEADS,\n",
    "    model_dim=MODEL_DIM,\n",
    "    vocab_size=hi_tokenizer.size,\n",
    "    pad_id=hi_tokenizer.specials[TOKEN_PAD],\n",
    "    context=DECODER_CONTEXT,\n",
    "    sos_id=hi_tokenizer.specials[TOKEN_SOS],\n",
    "    eos_id=hi_tokenizer.specials[TOKEN_EOS]\n",
    ")\n",
    "model = Transformer(encoder_config, decoder_config).to(DEVICE)\n",
    "# model.load_state_dict(torch.load(\"params.pth\", map_location=DEVICE))\n",
    "no_params = sum(p.nelement() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"model has {no_params / 1000 ** 2:.4f} million trainable parameters\")\n",
    "\n",
    "\n",
    "def save_model(t_loss, v_loss):\n",
    "    name = f\"{ENCODER_BLOCKS}_{DECODER_BLOCKS}__{ENCODER_HEADS}_{DECODER_HEADS}__{MODEL_DIM}__{t_loss:.2f}_{v_loss:.2f}.pth\"\n",
    "    torch.save(model.state_dict(), f\"drive/MyDrive/checkpoints/en-hi/{name}\")\n",
    "    print(f\"saved with t_loss: {t_loss:.2f}, v_loss: {v_loss:.2f}\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def calc_val_loss():\n",
    "    model.eval()\n",
    "    batch = val_dataset.next_batch()\n",
    "    if batch is None:\n",
    "        val_dataset.reset()\n",
    "        batch = val_dataset.next_batch()\n",
    "    x, y = batch\n",
    "    logits, loss = model(x, y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_monitor, good_delta= LossMonitor(\"train\", \"val\", window=200), None\n",
    "best_t_loss, best_v_loss = None, None\n",
    "assert None not in (good_delta, best_t_loss, best_v_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_trained, batches_trained = 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while True:\n",
    "    batch = train_dataset.next_batch()\n",
    "    if batch is None:\n",
    "        epochs_trained, batches_trained = epochs_trained + 1, 0\n",
    "        train_dataset.reset()\n",
    "        batch = train_dataset.next_batch()\n",
    "    x, y = batch\n",
    "    model.train()\n",
    "    logits, t_loss = model(x, y)\n",
    "    optimizer.zero_grad()\n",
    "    t_loss.backward()\n",
    "    optimizer.step()\n",
    "    t_loss, v_loss = t_loss.item(), calc_val_loss().item()\n",
    "    losses = loss_monitor.update(train=t_loss, val=v_loss)\n",
    "    batches_trained += 1\n",
    "    mt_loss, mv_loss = losses[\"train\"], losses[\"val\"]\n",
    "    print(f\"{epochs_trained}:{batches_trained} -> {mt_loss:.4f}, {mv_loss:.4f}\")\n",
    "    if best_t_loss - mt_loss >= good_delta and best_v_loss > mv_loss:\n",
    "        save_model(mt_loss, mv_loss)\n",
    "        best_t_loss, best_v_loss = t_loss, v_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient accumulation\n",
    "acc_t_loss = 0\n",
    "accumulation_steps = 3\n",
    "while True:\n",
    "    batch = train_dataset.next_batch()\n",
    "    if batch is None:\n",
    "        epochs_trained, batches_trained = epochs_trained + 1, 0\n",
    "        train_dataset.reset()\n",
    "        batch = train_dataset.next_batch()\n",
    "    x, y = batch\n",
    "    model.train()\n",
    "    logits, t_loss = model(x, y)\n",
    "    acc_t_loss += t_loss.item()\n",
    "    t_loss = t_loss / accumulation_steps\n",
    "    t_loss.backward()\n",
    "    batches_trained += 1\n",
    "    if batches_trained % accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        t_loss, v_loss = acc_t_loss / accumulation_steps, calc_val_loss().item()\n",
    "        losses = loss_monitor.update(train=t_loss, val=v_loss)\n",
    "        mt_loss, mv_loss = losses[\"train\"], losses[\"val\"]\n",
    "        print(f\"{epochs_trained}:{batches_trained // accumulation_steps} -> {mt_loss:.4f}, {mv_loss:.4f}\")\n",
    "        acc_t_loss = 0\n",
    "        if best_t_loss - mt_loss >= good_delta and mv_loss < best_v_loss:\n",
    "            save_model(mt_loss, mv_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(t_loss, v_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformer",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
