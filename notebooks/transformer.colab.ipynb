{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount(\"/content/drive\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/n1teshy/transformer > /dev/null\n",
    "!mv transformer/* . && rmdir transformer > /dev/null\n",
    "!ls drive/MyDrive/checkpoints/en-hi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !cp drive/MyDrive/checkpoints/en-hi/ params.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import AdamW\n",
    "from core.utils.bpe import Tokenizer\n",
    "from core.data.seq2seq import Dataset, Config as Seq2SeqConfig\n",
    "from core.utils.configs import EncoderConfig, DecoderConfig\n",
    "from core.models import Transformer\n",
    "from core.utils.loss import LossMonitor\n",
    "from core.globals import DEVICE\n",
    "from core.constants import TOKEN_PAD, TOKEN_SOS, TOKEN_EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN_SOURCE_FILE = \"./datasets/en-hi/train/en.txt\"\n",
    "# TRAIN_TARGET_FILE = \"./datasets/en-hi/train/hi.txt\"\n",
    "# VAL_SOURCE_FILE = \"./datasets/en-hi/val/en.txt\"\n",
    "# VAL_TARGET_FILE = \"./datasets/en-hi/val/hi.txt\"\n",
    "ENCODER_CONTEXT = 1024\n",
    "DECODER_CONTEXT = 512\n",
    "BATCH_SIZE = 64\n",
    "ENCODER_BLOCKS = 2\n",
    "ENCODER_HEADS = 4\n",
    "DECODER_BLOCKS = 2\n",
    "DECODER_HEADS = 4\n",
    "MODEL_DIM = 512\n",
    "\n",
    "assert MODEL_DIM % ENCODER_HEADS == MODEL_DIM % DECODER_HEADS == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_tokenizer = Tokenizer()\n",
    "en_tokenizer.load(\"tokenizers/en.model\")\n",
    "hi_tokenizer = Tokenizer()\n",
    "hi_tokenizer.load(\"tokenizers/hi.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.data.circular_loader import CircularDataloader\n",
    "\n",
    "# base_data_config = dict(\n",
    "#     # source=<source-file>,\n",
    "#     # target=<target-file>\n",
    "#     source_context=ENCODER_CONTEXT,\n",
    "#     target_context=DECODER_CONTEXT,\n",
    "#     encode_source=en_tokenizer.encode,\n",
    "#     encode_target=hi_tokenizer.encode,\n",
    "#     source_pad_id=en_tokenizer.specials[TOKEN_PAD],\n",
    "#     target_pad_id=hi_tokenizer.specials[TOKEN_PAD],\n",
    "#     sos_id=hi_tokenizer.specials[TOKEN_SOS],\n",
    "#     eos_id=hi_tokenizer.specials[TOKEN_EOS]\n",
    "# )\n",
    "\n",
    "train_dataset = Dataset()\n",
    "# train_dataset.prepare(\n",
    "#     Seq2SeqConfig(**dict(base_data_config, source=TRAIN_SOURCE_FILE, target=TRAIN_TARGET_FILE))\n",
    "# )\n",
    "train_dataset.load(\"drive/MyDrive/datasets/en-hi/train.pkl\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=train_dataset.collate)\n",
    "\n",
    "val_dataset = Dataset()\n",
    "# train_dataset.prepare(\n",
    "#     Seq2SeqConfig(**dict(base_data_config, source=VAL_SOURCE_FILE, target=VAL_TARGET_FILE))\n",
    "# )\n",
    "val_dataset.load(\"drive/MyDrive/datasets/en-hi/train.pkl\")\n",
    "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=val_dataset.collate)\n",
    "val_loader = iter(CircularDataloader(val_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_config = EncoderConfig(\n",
    "    no_blocks=ENCODER_BLOCKS,\n",
    "    no_heads=ENCODER_HEADS,\n",
    "    model_dim=MODEL_DIM,\n",
    "    vocab_size=en_tokenizer.size,\n",
    "    pad_id=en_tokenizer.specials[TOKEN_PAD],\n",
    "    context=ENCODER_CONTEXT\n",
    ")\n",
    "decoder_config = DecoderConfig(\n",
    "    no_blocks=DECODER_BLOCKS,\n",
    "    no_heads=DECODER_HEADS,\n",
    "    model_dim=MODEL_DIM,\n",
    "    vocab_size=hi_tokenizer.size,\n",
    "    pad_id=hi_tokenizer.specials[TOKEN_PAD],\n",
    "    context=DECODER_CONTEXT,\n",
    "    sos_id=hi_tokenizer.specials[TOKEN_SOS],\n",
    "    eos_id=hi_tokenizer.specials[TOKEN_EOS]\n",
    ")\n",
    "model = Transformer(encoder_config, decoder_config).to(DEVICE)\n",
    "# model.load_state_dict(torch.load(\"params.pth\", map_location=DEVICE))\n",
    "no_params = sum(p.nelement() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"model has {no_params / 1000 ** 2:.4f} million trainable parameters\")\n",
    "\n",
    "\n",
    "@torch.no_grad()\n",
    "def calc_val_loss():\n",
    "    model.eval()\n",
    "    x, y = next(val_loader)\n",
    "    logits, loss = model(x, y)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(model.parameters(), lr=0.0005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_monitor = LossMonitor(\"train\", \"val\", window=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "batches_trained = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for inp, tgt in train_loader:\n",
    "    model.train()\n",
    "    logits, t_loss = model(inp, tgt)\n",
    "    optimizer.zero_grad()\n",
    "    t_loss.backward()\n",
    "    optimizer.step()\n",
    "    t_loss, v_loss = t_loss.item(), calc_val_loss().item()\n",
    "    losses = loss_monitor.update(train=t_loss, val=v_loss)\n",
    "    batches_trained += 1\n",
    "    print(f\"{batches_trained} -> {t_loss:.4f}, {losses['train']:.4f}; {v_loss:.4f}, {losses['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gradient accumulation\n",
    "acc_t_loss = 0\n",
    "accumulation_steps = 3\n",
    "for inp, tgt in train_loader:\n",
    "    model.train()\n",
    "    logits, t_loss = model(inp, tgt)\n",
    "    acc_t_loss += t_loss.item()\n",
    "    t_loss = t_loss / accumulation_steps\n",
    "    t_loss.backward()\n",
    "    batches_trained += 1\n",
    "    if batches_trained % accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "        t_loss, v_loss = acc_t_loss / accumulation_steps, calc_val_loss().item()\n",
    "        losses = loss_monitor.update(train=t_loss, val=v_loss)\n",
    "        print(f\"{batches_trained // accumulation_steps} -> {t_loss:.4f}, {losses['train']:.4f}; {v_loss:.4f}, {losses['val']:.4f}\")\n",
    "        acc_t_loss = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "print(hi_tokenizer.decode(list(model.generate(torch.tensor([en_tokenizer.encode(\"\"\"Hello, how are you?\"\"\")])))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f\"drive/MyDrive/checkpoints/en-hi/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
